{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arya23-dev/ML/blob/main/1BM22CS055_LAB_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Step 2: Load the California housing dataset\n",
        "housing_data = fetch_california_housing()\n",
        "\n",
        "# Step 3: Convert to DataFrame\n",
        "df = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\n",
        "\n",
        "# Step 4: Add target variable (house prices)\n",
        "df[\"MedHouseVal\"] = housing_data.target\n",
        "\n",
        "# Step 5: Display information of all columns\n",
        "print(\"Dataset Information:\")\n",
        "print(df.info())\n",
        "\n",
        "# Step 6: Display statistical summary of numerical columns\n",
        "print(\"\\nStatistical Summary of Numerical Columns:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Step 7: Simulate a categorical column ('Ocean Proximity') for analysis\n",
        "df[\"Ocean Proximity\"] = pd.cut(df[\"MedHouseVal\"], bins=5, labels=[\"Near Bay\", \"Inland\", \"Island\", \"Near Ocean\", \"Ocean\"])\n",
        "print(\"\\nCount of Unique Labels in 'Ocean Proximity' Column:\")\n",
        "print(df[\"Ocean Proximity\"].value_counts())\n",
        "\n",
        "# Step 8: Display columns with missing values (California dataset does not have missing values)\n",
        "missing_values = df.isnull().sum()\n",
        "columns_with_missing = missing_values[missing_values > 0]\n",
        "\n",
        "print(\"\\nColumns with Missing Values:\")\n",
        "print(columns_with_missing if not columns_with_missing.empty else \"No missing values found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOI8E7OXblp-",
        "outputId": "d8719246-4723-41d7-edf7-560d6090e92a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20640 entries, 0 to 20639\n",
            "Data columns (total 9 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   MedInc       20640 non-null  float64\n",
            " 1   HouseAge     20640 non-null  float64\n",
            " 2   AveRooms     20640 non-null  float64\n",
            " 3   AveBedrms    20640 non-null  float64\n",
            " 4   Population   20640 non-null  float64\n",
            " 5   AveOccup     20640 non-null  float64\n",
            " 6   Latitude     20640 non-null  float64\n",
            " 7   Longitude    20640 non-null  float64\n",
            " 8   MedHouseVal  20640 non-null  float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 1.4 MB\n",
            "None\n",
            "\n",
            "Statistical Summary of Numerical Columns:\n",
            "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
            "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
            "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
            "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
            "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
            "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
            "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
            "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
            "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
            "\n",
            "           AveOccup      Latitude     Longitude   MedHouseVal  \n",
            "count  20640.000000  20640.000000  20640.000000  20640.000000  \n",
            "mean       3.070655     35.631861   -119.569704      2.068558  \n",
            "std       10.386050      2.135952      2.003532      1.153956  \n",
            "min        0.692308     32.540000   -124.350000      0.149990  \n",
            "25%        2.429741     33.930000   -121.800000      1.196000  \n",
            "50%        2.818116     34.260000   -118.490000      1.797000  \n",
            "75%        3.282261     37.710000   -118.010000      2.647250  \n",
            "max     1243.333333     41.950000   -114.310000      5.000010  \n",
            "\n",
            "Count of Unique Labels in 'Ocean Proximity' Column:\n",
            "Ocean Proximity\n",
            "Inland        7870\n",
            "Island        4568\n",
            "Near Bay      4489\n",
            "Near Ocean    1991\n",
            "Ocean         1722\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Columns with Missing Values:\n",
            "No missing values found\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Step 2: Load the Diabetes dataset (Ensure it's uploaded to Colab)\n",
        "file_path = \"Dataset_of_Diabetes.csv\"  # Update the path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 3: Handling Missing Values (Replacing NaNs with mean)\n",
        "# Create a copy of the DataFrame to avoid modifying the original DataFrame\n",
        "df_numeric = df.select_dtypes(include=['number']).copy() # Select only numeric columns\n",
        "\n",
        "# Initialize the SimpleImputer with the 'mean' strategy\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "\n",
        "# Apply the imputer to fill missing values in numeric columns only\n",
        "df_numeric.iloc[:, :] = imputer.fit_transform(df_numeric)\n",
        "\n",
        "# Merge the imputed numeric columns back into the original DataFrame\n",
        "df[df_numeric.columns] = df_numeric\n",
        "\n",
        "# Step 4: Handling Outliers (Removing values beyond 1.5*IQR)\n",
        "# Calculate quartiles and IQR for numeric columns\n",
        "Q1 = df_numeric.quantile(0.25) # Only compute quartiles on numeric data\n",
        "Q3 = df_numeric.quantile(0.75) # Only compute quartiles on numeric data\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Filter outliers from numeric columns and update the original DataFrame\n",
        "df = df[~((df_numeric < (Q1 - 1.5 * IQR)) | (df_numeric > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "# Step 5: Data Transformation\n",
        "# Min-Max Normalization (for numeric features)\n",
        "min_max_scaler = MinMaxScaler()\n",
        "df_minmax = pd.DataFrame(min_max_scaler.fit_transform(df_numeric), columns=df_numeric.columns) # Only transform the numeric columns\n",
        "\n",
        "# Standardization (Z-score normalization) (for numeric features)\n",
        "standard_scaler = StandardScaler()\n",
        "df_standard = pd.DataFrame(standard_scaler.fit_transform(df_numeric), columns=df_numeric.columns)  # Only transform the numeric columns\n",
        "\n",
        "# Step 6: Display Processed Data\n",
        "print(\"\\nProcessed Diabetes Dataset (Min-Max Scaled):\")\n",
        "print(df_minmax.head())\n",
        "\n",
        "print(\"\\nProcessed Diabetes Dataset (Standard Scaled):\")\n",
        "print(df_standard.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "wFsupRLccr3E",
        "outputId": "5ffcaf88-fa64-429a-ac3f-6182d15d3497"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Dataset_of_Diabetes.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fb65c3886a75>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Step 2: Load the Diabetes dataset (Ensure it's uploaded to Colab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Dataset_of_Diabetes.csv\"\u001b[0m  \u001b[0;31m# Update the path if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Step 3: Handling Missing Values (Replacing NaNs with mean)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset_of_Diabetes.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
        "\n",
        "# Step 2: Load the Adult Income dataset (Ensure it's uploaded to Colab)\n",
        "file_path = \"adult.csv\"  # Update the path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 3: Handling Missing Values (Replacing '?' with NaN and imputing mode for categorical, mean for numerical)\n",
        "df.replace(\"?\", np.nan, inplace=True)\n",
        "\n",
        "# Handling numerical missing values with mean\n",
        "num_imputer = SimpleImputer(strategy=\"mean\")\n",
        "df[df.select_dtypes(include=['number']).columns] = num_imputer.fit_transform(df.select_dtypes(include=['number']))\n",
        "\n",
        "# Handling categorical missing values with mode\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "df[df.select_dtypes(include=['object']).columns] = cat_imputer.fit_transform(df.select_dtypes(include=['object']))\n",
        "\n",
        "# Step 4: Handling Categorical Data (Encoding)\n",
        "label_encoders = {}\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Step 5: Handling Outliers (Removing values beyond 1.5*IQR)\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "# Step 6: Data Transformation\n",
        "# Min-Max Normalization\n",
        "min_max_scaler = MinMaxScaler()\n",
        "df_minmax = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# Standardization (Z-score normalization)\n",
        "standard_scaler = StandardScaler()\n",
        "df_standard = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# Step 7: Display Processed Data\n",
        "print(\"\\nProcessed Adult Income Dataset (Min-Max Scaled):\")\n",
        "print(df_minmax.head())\n",
        "\n",
        "print(\"\\nProcessed Adult Income Dataset (Standard Scaled):\")\n",
        "print(df_standard.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNPQp3Cpcu6t",
        "outputId": "90780bd4-30ee-4c62-dd8e-b5d332fa7d14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed Adult Income Dataset (Min-Max Scaled):\n",
            "        age  workclass    fnlwgt  education  educational-num  marital-status  \\\n",
            "0  0.344262        0.0  0.188277   0.555556         0.363636        0.333333   \n",
            "1  0.114754        0.0  0.881156   1.000000         0.454545        0.666667   \n",
            "2  0.147541        0.0  0.169156   0.555556         0.363636        0.666667   \n",
            "3  0.672131        0.0  0.708251   0.555556         0.363636        0.333333   \n",
            "4  0.131148        0.0  0.475807   0.333333         0.727273        0.333333   \n",
            "\n",
            "   occupation  relationship  race  gender  capital-gain  capital-loss  \\\n",
            "0    0.307692           0.0   0.0     1.0           0.0           0.0   \n",
            "1    0.538462           0.8   0.0     0.0           0.0           0.0   \n",
            "2    0.000000           0.2   0.0     0.0           0.0           0.0   \n",
            "3    0.692308           0.0   0.0     1.0           0.0           0.0   \n",
            "4    0.692308           0.0   0.0     1.0           0.0           0.0   \n",
            "\n",
            "   hours-per-week  native-country  income  \n",
            "0        0.894737             0.0     0.0  \n",
            "1        0.368421             0.0     0.0  \n",
            "2        0.315789             0.0     0.0  \n",
            "3        0.105263             0.0     0.0  \n",
            "4        0.368421             0.0     0.0  \n",
            "\n",
            "Processed Adult Income Dataset (Standard Scaled):\n",
            "        age  workclass    fnlwgt  education  educational-num  marital-status  \\\n",
            "0  0.220179        0.0 -1.022983  -0.151256        -0.654083       -0.398228   \n",
            "1 -0.955630        0.0  2.234629   1.457372        -0.073261        0.828047   \n",
            "2 -0.787657        0.0 -1.112882  -0.151256        -0.654083        0.828047   \n",
            "3  1.899906        0.0  1.421707  -0.151256        -0.654083       -0.398228   \n",
            "4 -0.871644        0.0  0.328856  -0.955571         1.669207       -0.398228   \n",
            "\n",
            "   occupation  relationship  race    gender  capital-gain  capital-loss  \\\n",
            "0   -0.420679     -1.044582   0.0  0.770972           0.0           0.0   \n",
            "1    0.305840      1.629927   0.0 -1.297064           0.0           0.0   \n",
            "2   -1.389371     -0.375955   0.0 -1.297064           0.0           0.0   \n",
            "3    0.790186     -1.044582   0.0  0.770972           0.0           0.0   \n",
            "4    0.790186     -1.044582   0.0  0.770972           0.0           0.0   \n",
            "\n",
            "   hours-per-week  native-country  income  \n",
            "0        2.312838             0.0     0.0  \n",
            "1       -0.329781             0.0     0.0  \n",
            "2       -0.594043             0.0     0.0  \n",
            "3       -1.651090             0.0     0.0  \n",
            "4       -0.329781             0.0     0.0  \n"
          ]
        }
      ]
    }
  ]
}